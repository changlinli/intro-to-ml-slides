# Transformer Conceptual Review Exercises

Let's first review some questions for 

> Exercise: Why do we need a positional embedding for our tokens in a
> transformer?

> Exercise: What pros and cons can you think of when it comes to using RoPE, a
> learned positional embedding, or the original 

> Exercise: Why does a decoder-only transformer require causal masking during
> training, but not during inference?
